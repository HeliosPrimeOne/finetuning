{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains some examples for how to use the finetune API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-07-21 20:45:16.231\u001b[0m | \u001b[1m      INFO      \u001b[0m | You are connecting to finney network with endpoint wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:45:16.233\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | We strongly encourage running a local subtensor node whenever possible. This increases decentralization and resilience of the network.\n",
      "\u001b[34m2024-07-21 20:45:16.233\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | In a future release, local subtensor will become the default endpoint. To get ahead of this change, please run a local subtensor node and point to it.\n",
      "\u001b[34m2024-07-21 20:45:17.222\u001b[0m | \u001b[1m      INFO      \u001b[0m | Connected to finney network and wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:45:25.817\u001b[0m | \u001b[1m      INFO      \u001b[0m | You are connecting to finney network with endpoint wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:45:25.818\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | We strongly encourage running a local subtensor node whenever possible. This increases decentralization and resilience of the network.\n",
      "\u001b[34m2024-07-21 20:45:25.818\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | In a future release, local subtensor will become the default endpoint. To get ahead of this change, please run a local subtensor node and point to it.\n",
      "\u001b[34m2024-07-21 20:45:26.408\u001b[0m | \u001b[1m      INFO      \u001b[0m | Connected to finney network and wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:45:26.410\u001b[0m | \u001b[1m      INFO      \u001b[0m | You are connecting to finney network with endpoint wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:45:26.411\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | We strongly encourage running a local subtensor node whenever possible. This increases decentralization and resilience of the network.\n",
      "\u001b[34m2024-07-21 20:45:26.411\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | In a future release, local subtensor will become the default endpoint. To get ahead of this change, please run a local subtensor node and point to it.\n",
      "\u001b[34m2024-07-21 20:45:26.992\u001b[0m | \u001b[1m      INFO      \u001b[0m | Connected to finney network and wss://entrypoint-finney.opentensor.ai:443.\n",
      "The best-performing model for SN9_MODEL competition is https://huggingface.co/MelancholyMist/MelancholyMaster/tree/e028d3d14e0f5e1128b32a66b180a91a30803c03\n"
     ]
    }
   ],
   "source": [
    "\"\"\"In this example, we load print the Hugging Face URL for the best-model in the SN9_MODEL competition.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "\n",
    "import finetune as ft\n",
    "from competitions.data import CompetitionId\n",
    "\n",
    "# Each model competes in a single competition. Find the best top performing miner UID for the \n",
    "# competition we care about (SN9_MODEL, in this example).\n",
    "top_model_uid = ft.graph.best_uid(competition_id=CompetitionId.SN9_MODEL)\n",
    "\n",
    "# Get the HuggingFace URL for this model.\n",
    "repo_url = asyncio.run(ft.mining.get_repo(top_model_uid))\n",
    "print(f\"The best-performing model for SN9_MODEL competition is {repo_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-07-21 20:48:23.609\u001b[0m | \u001b[1m      INFO      \u001b[0m | You are connecting to finney network with endpoint wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:48:23.610\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | We strongly encourage running a local subtensor node whenever possible. This increases decentralization and resilience of the network.\n",
      "\u001b[34m2024-07-21 20:48:23.610\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | In a future release, local subtensor will become the default endpoint. To get ahead of this change, please run a local subtensor node and point to it.\n",
      "\u001b[34m2024-07-21 20:48:24.602\u001b[0m | \u001b[1m      INFO      \u001b[0m | Connected to finney network and wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:48:33.468\u001b[0m | \u001b[1m      INFO      \u001b[0m | You are connecting to finney network with endpoint wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:48:33.469\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | We strongly encourage running a local subtensor node whenever possible. This increases decentralization and resilience of the network.\n",
      "\u001b[34m2024-07-21 20:48:33.469\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | In a future release, local subtensor will become the default endpoint. To get ahead of this change, please run a local subtensor node and point to it.\n",
      "\u001b[34m2024-07-21 20:48:34.079\u001b[0m | \u001b[1m      INFO      \u001b[0m | Connected to finney network and wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:48:41.427\u001b[0m | \u001b[1m      INFO      \u001b[0m | You are connecting to finney network with endpoint wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:48:41.427\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | We strongly encourage running a local subtensor node whenever possible. This increases decentralization and resilience of the network.\n",
      "\u001b[34m2024-07-21 20:48:41.427\u001b[0m | \u001b[33m\u001b[1m    WARNING     \u001b[0m | In a future release, local subtensor will become the default endpoint. To get ahead of this change, please run a local subtensor node and point to it.\n",
      "\u001b[34m2024-07-21 20:48:42.028\u001b[0m | \u001b[1m      INFO      \u001b[0m | Connected to finney network and wss://entrypoint-finney.opentensor.ai:443.\n",
      "\u001b[34m2024-07-21 20:48:44.591\u001b[0m | \u001b[32m\u001b[1m    SUCCESS     \u001b[0m | Fetched model metadata: ModelMetadata(id=ModelId(namespace='MelancholyMist', name='MelancholyMaster', competition_id=1, commit='e028d3d14e0f5e1128b32a66b180a91a30803c03', hash=None, secure_hash='ezMQ+VgL3Jhyxs2gAMTZ6SF3krL0jpkVpNp1H2Gowxo='), block=3435377)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:  67%|██████▋   | 2/3 [03:22<01:41, 101.64s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\"In this example, we load the top model for the SN9_MODEL competition and converse with it.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "\n",
    "import bittensor as bt\n",
    "import torch\n",
    "from taoverse.model.competition import utils as competition_utils\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "import constants\n",
    "import finetune as ft\n",
    "from competitions.data import CompetitionId\n",
    "\n",
    "# The device to run the model on.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Download the top model to the specified directory.\n",
    "download_dir = \"./finetune-example\"\n",
    "model = asyncio.run(\n",
    "    ft.mining.load_best_model(\n",
    "        download_dir=download_dir, competition_id=CompetitionId.SN9_MODEL\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load the competition so we can load the right tokenizer.\n",
    "metagraph = bt.metagraph(constants.SUBNET_UID)\n",
    "competition = competition_utils.get_competition_for_block(CompetitionId.SN9_MODEL, metagraph.block, constants.COMPETITION_SCHEDULE_BY_BLOCK)\n",
    "tokenizer = ft.model.load_tokenizer(competition.constraints)\n",
    "\n",
    "# Decide on a prompt.\n",
    "prompt = \"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "\n",
    "# Tokenize it.\n",
    "conversation = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=competition.constraints.sequence_length,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Generate the output.\n",
    "# You may wish to customize the generation config.\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=competition.constraints.sequence_length,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "with torch.inference_mode():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids, generation_config=generation_config\n",
    "    )\n",
    "    response = tokenizer.decode(\n",
    "        output[0][len(input_ids[0]) :], skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
